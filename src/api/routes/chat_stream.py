"""æµå¼å¯¹è¯æ¥å£è·¯ç”±ï¼ˆåŸºäº Callbacksï¼‰"""

from datetime import datetime
from typing import AsyncGenerator, Any, Dict, List
import asyncio
import json
from queue import Queue

from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from langchain_core.callbacks import BaseCallbackHandler

from src.agent.devops_agent import DevOpsAgent
from src.models.schemas import ChatRequest
from src.utils.logger import get_logger
from src.utils.agui_adapter import AGUIAdapter

logger = get_logger(__name__)
router = APIRouter(tags=["å¯¹è¯"])


class StreamingCallbackHandler(BaseCallbackHandler):
    """æµå¼å›è°ƒå¤„ç†å™¨

    æ•è· Agent æ‰§è¡Œè¿‡ç¨‹ä¸­çš„å„ç§äº‹ä»¶å¹¶æ”¾å…¥é˜Ÿåˆ—ã€‚
    """

    def __init__(self, queue: Queue):
        self.queue = queue

    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:
        """LLM å¼€å§‹"""
        self.queue.put({"type": "thinking", "content": "æ­£åœ¨æ€è€ƒ..."})

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """LLM ç”Ÿæˆæ–° token - å®ç°é€å­—æ˜¾ç¤º

        ğŸ”¥ æ”¯æŒ reasoning_contentï¼š
        - å¦‚æœ chunk ä¸­åŒ…å« reasoning_contentï¼Œå‘é€ reasoning_token äº‹ä»¶
        - å¦‚æœåŒ…å« contentï¼Œå‘é€æ™®é€š token äº‹ä»¶
        """
        # è·å– chunk å¯¹è±¡
        chunk = kwargs.get('chunk')

        # ğŸ”¥ æ£€æŸ¥æ˜¯å¦æœ‰ reasoning content
        if chunk:
            # chunk æ˜¯ ChatGenerationChunkï¼Œmessage æ˜¯ AIMessageChunk
            if hasattr(chunk, 'message'):
                msg = chunk.message
                # æ£€æŸ¥ additional_kwargs ä¸­çš„ reasoning_content
                if hasattr(msg, 'additional_kwargs') and msg.additional_kwargs:
                    reasoning = msg.additional_kwargs.get('reasoning_content')
                    if reasoning:
                        # å‘é€ reasoning token äº‹ä»¶
                        self.queue.put({
                            "type": "reasoning_token",
                            "content": reasoning
                        })
                        return  # reasoning token ä¸éœ€è¦å†å‘é€æ™®é€š token

        # å‘é€æ™®é€š content token
        self.queue.put({
            "type": "token",
            "content": token
        })

    def on_tool_start(self, serialized: Dict[str, Any], input_str: str, **kwargs: Any) -> None:
        """å·¥å…·è°ƒç”¨å¼€å§‹"""
        tool_name = serialized.get("name", "unknown")
        self.queue.put({
            "type": "tool_start",
            "tool": tool_name,
            "input": input_str[:200]
        })

    def on_tool_end(self, output: str, **kwargs: Any) -> None:
        """å·¥å…·è°ƒç”¨ç»“æŸ"""
        self.queue.put({
            "type": "tool_end",
            "output": output[:200]
        })

    def on_agent_action(self, action: Any, **kwargs: Any) -> None:
        """Agent è¡ŒåŠ¨"""
        self.queue.put({
            "type": "action",
            "action": action.tool,
            "thought": action.log[:300] if hasattr(action, 'log') else ""
        })

    def on_agent_finish(self, finish: Any, **kwargs: Any) -> None:
        """Agent å®Œæˆ"""
        output = finish.return_values.get("output", "") if hasattr(finish, 'return_values') else ""
        self.queue.put({
            "type": "done",
            "response": output
        })


async def stream_with_callback(agent: DevOpsAgent, message: str, session_id: str) -> AsyncGenerator[str, None]:
    """ä½¿ç”¨å›è°ƒçš„æµå¼è¿”å›ï¼ˆAG-UI åè®®ï¼‰"""
    queue = Queue()

    try:
        # åˆ›å»º AG-UI é€‚é…å™¨
        adapter = AGUIAdapter(session_id=session_id)

        # å‘é€ä¼šè¯å¼€å§‹äº‹ä»¶
        start_events = adapter.convert_event({"type": "start", "session_id": session_id or "new"})
        for event in start_events:
            yield f"data: {json.dumps(event, ensure_ascii=False)}\n\n"

        # åˆ›å»ºå›è°ƒå¤„ç†å™¨
        callback = StreamingCallbackHandler(queue)

        # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œ Agent
        def run_agent():
            try:
                executor, sid, memory = agent.create_executor(session_id)

                # æ·»åŠ å›è°ƒ
                result = executor.invoke(
                    {"input": message},
                    config={"callbacks": [callback]}
                )

                # æ”¾å…¥æœ€ç»ˆç»“æœ
                queue.put({
                    "type": "final",
                    "response": result.get("output", ""),
                    "session_id": sid
                })

            except Exception as e:
                queue.put({"type": "error", "error": str(e)})
            finally:
                queue.put(None)  # ç»“æŸæ ‡è®°

        # å¯åŠ¨åå°çº¿ç¨‹
        import threading
        thread = threading.Thread(target=run_agent)
        thread.start()

        # ä»é˜Ÿåˆ—è¯»å–å¹¶æµå¼å‘é€
        while True:
            await asyncio.sleep(0.1)  # é¿å…CPUå ç”¨è¿‡é«˜

            while not queue.empty():
                event = queue.get()

                if event is None:  # ç»“æŸæ ‡è®°
                    return

                # ğŸ”¥ é€šè¿‡ AG-UI é€‚é…å™¨è½¬æ¢äº‹ä»¶
                agui_events = adapter.convert_event(event)

                # å‘é€è½¬æ¢åçš„äº‹ä»¶ï¼ˆå¯èƒ½æ˜¯å¤šä¸ªï¼‰
                for agui_event in agui_events:
                    yield f"data: {json.dumps(agui_event, ensure_ascii=False)}\n\n"

    except Exception as e:
        logger.error(f"æµå¼å“åº”é”™è¯¯: {str(e)}")
        # å‘é€é”™è¯¯äº‹ä»¶
        error_events = adapter.convert_event({"type": "error", "error": str(e)})
        for event in error_events:
            yield f"data: {json.dumps(event, ensure_ascii=False)}\n\n"


@router.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """æµå¼å¯¹è¯æ¥å£ï¼ˆAG-UI åè®®ï¼‰

    ç¬¦åˆ AG-UI åè®®æ ‡å‡†çš„æµå¼ SSE æ¥å£ï¼Œæ”¯æŒ TDesign Chat ç»„ä»¶ç›´æ¥é›†æˆã€‚

    AG-UI äº‹ä»¶ç±»å‹ï¼š
    - RUN_STARTED/FINISHED/ERROR: ä¼šè¯ç”Ÿå‘½å‘¨æœŸ
    - THINKING_*: æ€è€ƒè¿‡ç¨‹ï¼ˆDeepseek Reasoningï¼‰
    - TEXT_MESSAGE_*: æ­£å¼å›ç­”å†…å®¹
    - TOOL_CALL_*: å·¥å…·è°ƒç”¨è¿‡ç¨‹

    å‰ç«¯é›†æˆç¤ºä¾‹ï¼ˆVue3 + TDesign Chatï¼‰ï¼š
    ```vue
    <template>
      <t-chatbot :chat-service-config="chatServiceConfig" />
    </template>

    <script setup>
    const chatServiceConfig = {
      endpoint: '/api/v1/chat/stream',
      protocol: 'agui',  // å¯ç”¨ AG-UI åè®®
      stream: true,
    };
    </script>
    ```

    æˆ–ä½¿ç”¨åŸç”Ÿ EventSourceï¼š
    ```javascript
    const eventSource = new EventSource('/api/v1/chat/stream');
    eventSource.onmessage = (event) => {
        const data = JSON.parse(event.data);
        console.log(data.type, data);
    };
    ```

    æˆ–ä½¿ç”¨ curl æµ‹è¯•ï¼š
    ```bash
    curl -N -X POST http://localhost:8000/api/v1/chat/stream \\
      -H "Content-Type: application/json" \\
      -d '{"message": "æŸ¥è¯¢é¡¹ç›®çŠ¶æ€", "session_id": null}'
    ```
    """
    agent = DevOpsAgent()

    return StreamingResponse(
        stream_with_callback(agent, request.message, request.session_id),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )
